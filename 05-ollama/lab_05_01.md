## Writing Applications with Ollama


1. You can also use the `ollama` Python package to leverage the Ollama API with Python.  Install it from the terminal panel with `pip`
    ```bash
    pip install ollama
    ```
2. Now we need to configure a model to write an application against.  You saw how to do this in the exercises but the problem is when the model stops running, you lose the system prompt and parameters.  You can create a model with custom system messages and parameters by defining them in a Modelfile.  Create a new file in `05-ollama` and call it `Modelfile`.
3. The first line of the Modelfile defines the base model, `ollama3.2:1b`.
    ```
    FROM ollama3.2:1b
    ```
    If you are asking is a Modelfile similar to a Dockerfile, the answer is yes.  A Dockerfile defines the configuration for an image and then you build the image from the Dockerfile using the Docker CLI.  You use a Modelfile to define system messages and parameter for an Ollama model and then create a new model using the Ollama CLI.  
4. Now define a `PARAMETER` for `temperature` and `top_p`.
    ```
    PARAMETER temperature 0.3
    PARAMETER top_p 0.7
    ```
5. And a `SYSTEM` for the system prompt.  Copy the prompt from `03-github-models/customer_service_prompt.txt`.
    ```
    SYSTEM [paste prompt]
    ```
6. Save the `Modelfile` and go back to the terminal panel.  Run the `create` command to create the new model with the name `customer_service`.
    ```bash
    ollama create customer_service
    ```
    The Ollama CLI will by default look for a `Modelfile` in the same folder where the command was run.
7. Run the `ollama ls` command to see the new `customer_service` model.
8. And run the model
    ```bash
    ollama run customer_service
    ```
9. Send it a prompt that has to do with customer service like: `I accidentally ordered the wrong item. How can I get a refund?`
10. Now create a new Jupyter notebook in `05-ollama` and name it `chat.ipynb`.
    * *Ctrl/Cmd-Shift-P* to open the Command Pallette
    * Search for `jupyter` and select **Create: New Jupyter Notebook**
11. In the first cell, import the `chat` function and the `ChatResponse` type from the `ollama` module and execute the cell.
    ```python
    from ollama import chat, ChatResponse
    ```
12. Define the `messages` list in the next cell
    ```python
    messages = [
        {
            "role": "user",
            "content": "I ordered the wrong item.  How can I get a refund?"
        }
    ]
    ```
13. Call the `chat` function passing it the model name (`customer_service`), the `messages` and a keyword argument `stream` set to `True`.
    ```python
    content: ChatResponse = chat(
        "customer_service",
        messages=messages,
        stream=True
    )
    ```
    Execute the cell.  Notice it finishes right away.  This is because it won't start generating the response until the `content` is parsed.
14. Iterate over the `content`.  For each `chunk`, print the `content` of the `message`.  Also add the `end` and `flush` keywords arguments to take advantage of streaming.
    ```python
    for chunk in content:
        print(chunk.message.content, end="", flush=True)
    ```
    Execute the cell and after a brief delay (if the model has to start up again) you'll see the response displayed as it is streamed from the server.