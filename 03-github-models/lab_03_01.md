## Support Ticket Classifier


In this lab, you will use GitHub Models and the OpenAI SDK to write a simple application which analyzes a technical support ticket.  An LLM will then summarize the issue and assign it a label.  This data will be returned in a JSON object that can be consumed by the `json` module from the Python standard library.

---
1. In `03-github-models` create a new file named `tickets.py`.
2. import the `json` and `os` modules as well as the `OpenAI` client class from the `openai` module.
    ```python
    import json
    import os

    from openai import OpenAI
    ```
3. Retrieve your personal access token from the `GH_MODELS_TOKEN` environment variable
    ```python
    token = os.getenv("GH_MODELS_TOKEN", "")
    ```
4. Create an instance of the OpenAI client class.
    ```python
    client = OpenAI(
        base_url="https://models.github.ai/inference",
        api_key=token
    )
    ```
5. In `03-github-models` there is a `lab_prompts.py` file.  Inside is a dictionary with three keys.  
    * The first one, `SYSTEM` is a system prompt telling the LLM to act as a customer support assistant and for each ticket, return a JSON object with a category and summary.  
    * The second prompt, `USER` is an example support ticket.  
    * The third, `ASSISTANT` is a possible response for the `USER` message.  Notice it uses the `json.dumps` method to ensure the response is a valid JSON object.  
    Import the `lab_prompts` module and construct the messages that will be sent to the LLM.
    ```python
    from lab_prompts import prompts
    system_message = {
        "role": "system",
        "content": prompts["SYSTEM"]
    }
    user_message = {
        "role": "user",
        "content": prompts["USER"]
    }
    assistant_message = {
        "role": "assistant",
        "content": prompts["ASSISTANT"]
    }
    ```
6. Iterate over the sample tickets in the `SAMPLES` key in the `lab_prompts` module
    ```python
    for ticket in prompts["SAMPLES"]:

    ```
7. Create the `messages` list from the system prompt and the example interaction.
    ```python
        messages = [system_message, user_message, assistant_message]
    ```
8. Append a new user message dictionary to serve as the prompt
    ```python
        messages.append({
            "role": "user",
            "content": ticket
        })
    ```
9. Generate the response
    ```python
        response = client.chat.completions.create(
            messages=messages,
            temperature=0.7,
            max_tokens=750,
            model="openai/gpt-4.1-mini",
        )
    ```
10. The content of the response may need to be cleaned up a little.  
    * First strip all leading and trailing whitespace
    * Some models surround the JSON object with markdown.  The second line selects only the content from the opening brace to the closing.
    ```python
        content = response.choices[0].message.content.strip()
        content = content[content.index("{"):content.rindex("}") + 1]
    ```
11. Use the `json.loads` function to convert the JSON object into a Python dictionary
    ```python
        summary = json.loads(content)
    ```
12. Display the category and summary
    ```python
        print("Category: ", summary["category"])
        print("Summary: ", summary["summary"])
    print("-" * 60)
    ```
